Executive Presentation: How We Detect Active Devices & Health Anomalies
“What’s Running Where” + Device Health Anomaly Detection
1. Why Simple Thresholds (e.g., CPU > 80%) Are Not Enough
Applications have different normal behaviors:
A batch job may normally run at 90% CPU for hours → healthy.
A web server idling at 5% CPU most of the day, then spiking to 70% during traffic → also healthy.
Traffic is seasonal: Higher during business hours, lower at night/weekends.
Fixed thresholds cause:
– Too many false alerts (alert fatigue)
– Missed real issues (e.g., slow memory leak)
Solution: Dynamic, per-device baselines + intelligent thresholds.
2. Core Algorithm: Dynamic Baseline + Smart Thresholds
We compare current load against a learned baseline for each device/service.
Step-by-Step Algorithm
Step
What We Do
Why It Matters
Example
1. Define “Current Load”
Average of (CPU % + Memory %) / 2 over the last 12 minutes
Short window captures the present state without noise from single spikes
Device shows CPU 75%, Memory 60% → Current Load = 67.5%
2. Compute “Baseline Load”
Average of same combined metric from 90–102 minutes ago
This period is far enough to avoid contamination from recent activity, yet recent enough to reflect normal patterns (includes daily cycle)
95 minutes ago, the device was idle → Baseline Load = 8%
3. Calculate Deviation (Load Delta)
Current Load – Baseline Load
Measures how much higher (or lower) usage is compared to normal
67.5% – 8% = +59.5% delta
4. Apply Smart Thresholds
See rules below
Avoids false positives/negatives
—
3. Threshold Rules – Explained with Real-World Examples
We use three zones instead of one fixed number:
Zone
Rule
Interpretation
Example Scenario
Decision
Why This Threshold?
Clearly Idle
Current Load ≤ 10%
Device is essentially doing nothing
Night-time cache server: Current Load 6%
Inactive
Even with some background processes, <10% is true idle across most workloads
High Spike (Clear Anomaly)
Current Load ≥ Baseline + 15%
Significant unexpected increase
Web server: Baseline 12%, Current 65% → Delta +53%
Active + Potential Anomaly
15% delta is large enough to indicate real traffic or issue; smaller fluctuations are normal
Moderate / Sustained
Everything else (Current > 10% AND not a high spike)
Device is working at expected level
Background worker: Baseline 25%, Current 35% → Delta +10%
Active (Healthy)
Sustained moderate load is normal for many services — we treat as active
Key Principle:
We prefer to over-report “active” rather than miss a running service.
Only mark inactive when we are confident the device is truly idle (<10%).
4. Real-World Examples
Example 1: Healthy Web Server During Traffic Peak
Baseline (quiet period): 10% combined
Current (mid-day traffic): 68% combined
Delta: +58% → High spike zone
→ Marked Active (correct)
→ Also flagged for anomaly review (optional alert)
Example 2: Batch Job Processor (Always Busy)
Baseline (normal operation): 70%
Current: 78%
Delta: +8% → Moderate zone
→ Marked Active (correct)
→ No anomaly (within normal variation)
Example 3: Idle Standby Server at Night
Baseline: 8%
Current: 7%
Delta: -1% → Clearly Idle zone
→ Marked Inactive (correct)
Example 4: Memory Leak Beginning
Baseline: 30%
Current: 62% (gradual rise over hours)
Delta: +32% → High spike zone
→ Marked Active + Anomaly flagged
→ Ops team investigates → catches leak early
5. Anomaly Detection Extension (Proactive Health)
We add one more layer on top:
Compute standard deviation of load in the recent 12-minute window
If deviation from baseline > 3 × recent standard deviation → Strong anomaly
Example:
Baseline: 15%
Current: 65%
Recent std dev: 8%
3σ = 24% → 65% – 15% = 50% > 24% → Anomaly alert
This catches:
Sudden crashes/spikes
Unusual drops (e.g., process died)
Sustained abnormal levels
6. Benefits for the Business
Benefit
Impact
Accurate “What’s Running Where” view
Engineers and managers see true live footprint across 200k devices
Reduced alert fatigue
Only meaningful anomalies surface
Early problem detection
Memory leaks, stuck processes caught before outages
No manual threshold tuning per app
Algorithm adapts automatically per device/service
Scalable & fast
Runs in MongoDB in <500ms per app — even at 100M+ metrics/day
Summary Slide Message
“We no longer guess if a service is running or healthy using fixed rules.
Instead, each device learns its own normal behavior.
We only say ‘inactive’ when truly idle (<10%).
We say ‘active’ for moderate or high load — and flag only significant deviations as anomalies.
This gives us an accurate, real-time map of our live infrastructure — with early warnings when something is wrong.”
This approach is proven, scalable, and already running efficiently in our MongoDB time-series platform.
